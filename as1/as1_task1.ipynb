{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322864f7",
   "metadata": {},
   "source": [
    "### To DO:\n",
    "- Hyperparameters for optimizers\n",
    "- Possibly test all parameters at the same time\n",
    "- Apply best to CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdb2b764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow[and-cuda] in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (2.13.1)\n",
      "\u001b[33mWARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow-macos==2.13.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow[and-cuda]) (2.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.13.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/vanbuncha/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.1->tensorflow[and-cuda]) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#! pip install keras\n",
    "#! pip install tensorflow\n",
    "#! pip install torch\n",
    "! pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a83a6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras\n",
    "from __future__ import print_function\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# initializers\n",
    "from keras.initializers import Zeros  \n",
    "from keras.initializers import RandomNormal, RandomUniform  \n",
    "from keras.initializers import glorot_normal, glorot_uniform \n",
    "from keras.initializers import he_normal, he_uniform  \n",
    "from keras.initializers import lecun_normal, lecun_uniform  \n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad34711",
   "metadata": {},
   "source": [
    "# Task 1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6373974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_468\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1268 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_816 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1269 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_817 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1270 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2485 - accuracy: 0.9234 - val_loss: 0.1005 - val_accuracy: 0.9679\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1041 - accuracy: 0.9685 - val_loss: 0.0859 - val_accuracy: 0.9730\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0750 - accuracy: 0.9772 - val_loss: 0.0775 - val_accuracy: 0.9757\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0612 - accuracy: 0.9814 - val_loss: 0.0869 - val_accuracy: 0.9780\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0532 - accuracy: 0.9845 - val_loss: 0.0703 - val_accuracy: 0.9818\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0424 - accuracy: 0.9875 - val_loss: 0.0771 - val_accuracy: 0.9817\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.0799 - val_accuracy: 0.9814\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0361 - accuracy: 0.9895 - val_loss: 0.0742 - val_accuracy: 0.9842\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0302 - accuracy: 0.9911 - val_loss: 0.0848 - val_accuracy: 0.9842\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.0927 - val_accuracy: 0.9820\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.1013 - val_accuracy: 0.9813\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0254 - accuracy: 0.9928 - val_loss: 0.0910 - val_accuracy: 0.9845\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0222 - accuracy: 0.9936 - val_loss: 0.1078 - val_accuracy: 0.9814\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0215 - accuracy: 0.9940 - val_loss: 0.1009 - val_accuracy: 0.9841\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1159 - val_accuracy: 0.9820\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0197 - accuracy: 0.9945 - val_loss: 0.1124 - val_accuracy: 0.9841\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0203 - accuracy: 0.9951 - val_loss: 0.1098 - val_accuracy: 0.9831\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 0.1172 - val_accuracy: 0.9838\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 0.1223 - val_accuracy: 0.9833\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0163 - accuracy: 0.9956 - val_loss: 0.1361 - val_accuracy: 0.9821\n",
      "Test loss: 0.1360960602760315\n",
      "Test accuracy: 0.9821000099182129\n"
     ]
    }
   ],
   "source": [
    "# mnist_mlp.py\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40c74b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/12\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 2.2884 - accuracy: 0.1390 - val_loss: 2.2598 - val_accuracy: 0.3389\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 26s 56ms/step - loss: 2.2399 - accuracy: 0.2646 - val_loss: 2.1994 - val_accuracy: 0.5681\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 28s 59ms/step - loss: 2.1756 - accuracy: 0.3754 - val_loss: 2.1153 - val_accuracy: 0.6516\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 30s 63ms/step - loss: 2.0825 - accuracy: 0.4604 - val_loss: 1.9934 - val_accuracy: 0.6958\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 27s 59ms/step - loss: 1.9540 - accuracy: 0.5204 - val_loss: 1.8259 - val_accuracy: 0.7227\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 1.7873 - accuracy: 0.5707 - val_loss: 1.6142 - val_accuracy: 0.7482\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 27s 58ms/step - loss: 1.5935 - accuracy: 0.6045 - val_loss: 1.3810 - val_accuracy: 0.7729\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 29s 61ms/step - loss: 1.4035 - accuracy: 0.6372 - val_loss: 1.1650 - val_accuracy: 0.7935\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 1.2465 - accuracy: 0.6556 - val_loss: 0.9930 - val_accuracy: 0.8150\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 31s 65ms/step - loss: 1.1220 - accuracy: 0.6794 - val_loss: 0.8654 - val_accuracy: 0.8260\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 1.0237 - accuracy: 0.7003 - val_loss: 0.7703 - val_accuracy: 0.8383\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 29s 63ms/step - loss: 0.9490 - accuracy: 0.7146 - val_loss: 0.6999 - val_accuracy: 0.8464\n",
      "Test loss: 0.6998873353004456\n",
      "Test accuracy: 0.8464000225067139\n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f2ee6",
   "metadata": {},
   "source": [
    "# Task  1.2 MLP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f5ad813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_470\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1273 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_820 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1274 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_821 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1275 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3017 - accuracy: 0.1121 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "Model: \"sequential_471\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1276 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_822 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1277 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_823 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1278 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/1700375211.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2520 - accuracy: 0.9224 - val_loss: 0.1090 - val_accuracy: 0.9638\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1045 - accuracy: 0.9680 - val_loss: 0.0774 - val_accuracy: 0.9763\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0774 - accuracy: 0.9769 - val_loss: 0.0773 - val_accuracy: 0.9779\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0607 - accuracy: 0.9818 - val_loss: 0.0744 - val_accuracy: 0.9797\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0506 - accuracy: 0.9853 - val_loss: 0.0765 - val_accuracy: 0.9794\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0818 - val_accuracy: 0.9800\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0366 - accuracy: 0.9891 - val_loss: 0.0811 - val_accuracy: 0.9812\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 0.0804 - val_accuracy: 0.9823\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0311 - accuracy: 0.9908 - val_loss: 0.0805 - val_accuracy: 0.9829\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0265 - accuracy: 0.9919 - val_loss: 0.0969 - val_accuracy: 0.9827\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 0.0832 - val_accuracy: 0.9849\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0228 - accuracy: 0.9934 - val_loss: 0.0970 - val_accuracy: 0.9831\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0229 - accuracy: 0.9932 - val_loss: 0.0991 - val_accuracy: 0.9826\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.0961 - val_accuracy: 0.9841\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0197 - accuracy: 0.9944 - val_loss: 0.1062 - val_accuracy: 0.9831\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.1050 - val_accuracy: 0.9837\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.1287 - val_accuracy: 0.9825\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.1153 - val_accuracy: 0.9833\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0170 - accuracy: 0.9952 - val_loss: 0.1181 - val_accuracy: 0.9830\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.1142 - val_accuracy: 0.9831\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "Model: \"sequential_472\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1279 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_824 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1280 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_825 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1281 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2692 - accuracy: 0.9172 - val_loss: 0.1165 - val_accuracy: 0.9632\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1069 - accuracy: 0.9685 - val_loss: 0.0860 - val_accuracy: 0.9729\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0758 - accuracy: 0.9769 - val_loss: 0.0773 - val_accuracy: 0.9784\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0614 - accuracy: 0.9812 - val_loss: 0.0796 - val_accuracy: 0.9803\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0507 - accuracy: 0.9849 - val_loss: 0.0702 - val_accuracy: 0.9819\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0442 - accuracy: 0.9868 - val_loss: 0.0783 - val_accuracy: 0.9803\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0377 - accuracy: 0.9891 - val_loss: 0.0788 - val_accuracy: 0.9817\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0331 - accuracy: 0.9900 - val_loss: 0.0845 - val_accuracy: 0.9813\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0331 - accuracy: 0.9907 - val_loss: 0.0891 - val_accuracy: 0.9802\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0282 - accuracy: 0.9919 - val_loss: 0.0804 - val_accuracy: 0.9838\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0255 - accuracy: 0.9927 - val_loss: 0.1017 - val_accuracy: 0.9807\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0263 - accuracy: 0.9925 - val_loss: 0.1105 - val_accuracy: 0.9817\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0247 - accuracy: 0.9931 - val_loss: 0.0928 - val_accuracy: 0.9842\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0235 - accuracy: 0.9934 - val_loss: 0.1038 - val_accuracy: 0.9825\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0210 - accuracy: 0.9940 - val_loss: 0.1073 - val_accuracy: 0.9813\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0209 - accuracy: 0.9944 - val_loss: 0.1139 - val_accuracy: 0.9832\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0173 - accuracy: 0.9952 - val_loss: 0.1158 - val_accuracy: 0.9832\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0196 - accuracy: 0.9945 - val_loss: 0.1319 - val_accuracy: 0.9823\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0179 - accuracy: 0.9952 - val_loss: 0.1129 - val_accuracy: 0.9856\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0162 - accuracy: 0.9958 - val_loss: 0.1106 - val_accuracy: 0.9845\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "Model: \"sequential_473\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1282 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_826 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1283 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_827 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1284 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2442 - accuracy: 0.9253 - val_loss: 0.1120 - val_accuracy: 0.9668\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1026 - accuracy: 0.9691 - val_loss: 0.0870 - val_accuracy: 0.9726\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0761 - accuracy: 0.9772 - val_loss: 0.0814 - val_accuracy: 0.9764\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0596 - accuracy: 0.9820 - val_loss: 0.0729 - val_accuracy: 0.9789\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0516 - accuracy: 0.9846 - val_loss: 0.0714 - val_accuracy: 0.9815\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0443 - accuracy: 0.9872 - val_loss: 0.0748 - val_accuracy: 0.9819\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0383 - accuracy: 0.9886 - val_loss: 0.0748 - val_accuracy: 0.9822\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0352 - accuracy: 0.9895 - val_loss: 0.0809 - val_accuracy: 0.9827\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0308 - accuracy: 0.9911 - val_loss: 0.0870 - val_accuracy: 0.9834\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.0950 - val_accuracy: 0.9800\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0285 - accuracy: 0.9919 - val_loss: 0.0856 - val_accuracy: 0.9830\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0261 - accuracy: 0.9929 - val_loss: 0.1083 - val_accuracy: 0.9818\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0247 - accuracy: 0.9929 - val_loss: 0.1042 - val_accuracy: 0.9829\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0234 - accuracy: 0.9934 - val_loss: 0.1004 - val_accuracy: 0.9832\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0221 - accuracy: 0.9940 - val_loss: 0.1015 - val_accuracy: 0.9829\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0227 - accuracy: 0.9938 - val_loss: 0.0985 - val_accuracy: 0.9827\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0207 - accuracy: 0.9940 - val_loss: 0.1099 - val_accuracy: 0.9833\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0194 - accuracy: 0.9950 - val_loss: 0.1277 - val_accuracy: 0.9821\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0195 - accuracy: 0.9949 - val_loss: 0.1275 - val_accuracy: 0.9832\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0192 - accuracy: 0.9947 - val_loss: 0.1196 - val_accuracy: 0.9824\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "3         GlorotUniform   0.119634         0.9824\n",
      "Model: \"sequential_474\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1285 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_828 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1286 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_829 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1287 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2477 - accuracy: 0.9238 - val_loss: 0.1048 - val_accuracy: 0.9688\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1034 - accuracy: 0.9689 - val_loss: 0.0897 - val_accuracy: 0.9734\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0747 - accuracy: 0.9771 - val_loss: 0.0754 - val_accuracy: 0.9793\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0594 - accuracy: 0.9820 - val_loss: 0.0757 - val_accuracy: 0.9790\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0490 - accuracy: 0.9851 - val_loss: 0.0790 - val_accuracy: 0.9802\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0434 - accuracy: 0.9869 - val_loss: 0.0822 - val_accuracy: 0.9802\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0373 - accuracy: 0.9892 - val_loss: 0.0800 - val_accuracy: 0.9809\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0910 - val_accuracy: 0.9815\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0306 - accuracy: 0.9909 - val_loss: 0.0944 - val_accuracy: 0.9829\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.0933 - val_accuracy: 0.9824\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0269 - accuracy: 0.9922 - val_loss: 0.1037 - val_accuracy: 0.9816\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0255 - accuracy: 0.9927 - val_loss: 0.1080 - val_accuracy: 0.9815\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0235 - accuracy: 0.9932 - val_loss: 0.0874 - val_accuracy: 0.9852\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 0.1170 - val_accuracy: 0.9833\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.1104 - val_accuracy: 0.9834\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0180 - accuracy: 0.9947 - val_loss: 0.1175 - val_accuracy: 0.9842\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.1203 - val_accuracy: 0.9832\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0168 - accuracy: 0.9956 - val_loss: 0.1115 - val_accuracy: 0.9852\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0164 - accuracy: 0.9957 - val_loss: 0.1399 - val_accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0185 - accuracy: 0.9953 - val_loss: 0.1299 - val_accuracy: 0.9844\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "3         GlorotUniform   0.119634         0.9824\n",
      "4          GlorotNormal   0.129930         0.9844\n",
      "Model: \"sequential_475\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1288 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_830 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1289 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_831 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1290 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2509 - accuracy: 0.9230 - val_loss: 0.1378 - val_accuracy: 0.9585\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1049 - accuracy: 0.9681 - val_loss: 0.0967 - val_accuracy: 0.9719\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0754 - accuracy: 0.9778 - val_loss: 0.0904 - val_accuracy: 0.9734\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0601 - accuracy: 0.9814 - val_loss: 0.0827 - val_accuracy: 0.9773\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0529 - accuracy: 0.9844 - val_loss: 0.0669 - val_accuracy: 0.9808\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0435 - accuracy: 0.9865 - val_loss: 0.0710 - val_accuracy: 0.9837\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0379 - accuracy: 0.9886 - val_loss: 0.0884 - val_accuracy: 0.9812\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0342 - accuracy: 0.9903 - val_loss: 0.0683 - val_accuracy: 0.9849\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0297 - accuracy: 0.9912 - val_loss: 0.0833 - val_accuracy: 0.9817\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0297 - accuracy: 0.9912 - val_loss: 0.0859 - val_accuracy: 0.9827\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0267 - accuracy: 0.9920 - val_loss: 0.0894 - val_accuracy: 0.9811\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0231 - accuracy: 0.9935 - val_loss: 0.0957 - val_accuracy: 0.9835\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0234 - accuracy: 0.9931 - val_loss: 0.0973 - val_accuracy: 0.9812\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0210 - accuracy: 0.9941 - val_loss: 0.1098 - val_accuracy: 0.9814\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0219 - accuracy: 0.9938 - val_loss: 0.0991 - val_accuracy: 0.9818\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9943 - val_loss: 0.1018 - val_accuracy: 0.9838\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0189 - accuracy: 0.9948 - val_loss: 0.1155 - val_accuracy: 0.9825\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.1237 - val_accuracy: 0.9823\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0178 - accuracy: 0.9950 - val_loss: 0.1197 - val_accuracy: 0.9817\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.1244 - val_accuracy: 0.9829\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "3         GlorotUniform   0.119634         0.9824\n",
      "4          GlorotNormal   0.129930         0.9844\n",
      "5              HeNormal   0.124419         0.9829\n",
      "Model: \"sequential_476\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1291 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_832 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1292 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_833 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1293 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2487 - accuracy: 0.9238 - val_loss: 0.1032 - val_accuracy: 0.9679\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1032 - accuracy: 0.9687 - val_loss: 0.0987 - val_accuracy: 0.9715\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0744 - accuracy: 0.9774 - val_loss: 0.0730 - val_accuracy: 0.9780\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0602 - accuracy: 0.9816 - val_loss: 0.0842 - val_accuracy: 0.9767\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0509 - accuracy: 0.9852 - val_loss: 0.0727 - val_accuracy: 0.9805\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0444 - accuracy: 0.9867 - val_loss: 0.0780 - val_accuracy: 0.9810\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0370 - accuracy: 0.9889 - val_loss: 0.0804 - val_accuracy: 0.9804\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.0784 - val_accuracy: 0.9824\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0325 - accuracy: 0.9909 - val_loss: 0.0812 - val_accuracy: 0.9820\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0275 - accuracy: 0.9921 - val_loss: 0.0912 - val_accuracy: 0.9837\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0251 - accuracy: 0.9927 - val_loss: 0.0913 - val_accuracy: 0.9818\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0246 - accuracy: 0.9933 - val_loss: 0.0875 - val_accuracy: 0.9831\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0218 - accuracy: 0.9937 - val_loss: 0.0940 - val_accuracy: 0.9841\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0223 - accuracy: 0.9933 - val_loss: 0.0964 - val_accuracy: 0.9853\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0203 - accuracy: 0.9943 - val_loss: 0.1045 - val_accuracy: 0.9827\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.1108 - val_accuracy: 0.9822\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0187 - accuracy: 0.9948 - val_loss: 0.1046 - val_accuracy: 0.9832\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.1139 - val_accuracy: 0.9828\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.1205 - val_accuracy: 0.9831\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0175 - accuracy: 0.9958 - val_loss: 0.1380 - val_accuracy: 0.9821\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "3         GlorotUniform   0.119634         0.9824\n",
      "4          GlorotNormal   0.129930         0.9844\n",
      "5              HeNormal   0.124419         0.9829\n",
      "6             HeUniform   0.137970         0.9821\n",
      "Model: \"sequential_477\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1294 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_834 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1295 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_835 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1296 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2502 - accuracy: 0.9222 - val_loss: 0.1489 - val_accuracy: 0.9535\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1022 - accuracy: 0.9687 - val_loss: 0.0873 - val_accuracy: 0.9741\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0760 - accuracy: 0.9771 - val_loss: 0.0757 - val_accuracy: 0.9801\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0607 - accuracy: 0.9815 - val_loss: 0.0714 - val_accuracy: 0.9817\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 0.0705 - val_accuracy: 0.9815\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0433 - accuracy: 0.9869 - val_loss: 0.0691 - val_accuracy: 0.9825\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0389 - accuracy: 0.9888 - val_loss: 0.0869 - val_accuracy: 0.9801\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0345 - accuracy: 0.9898 - val_loss: 0.0851 - val_accuracy: 0.9818\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0312 - accuracy: 0.9905 - val_loss: 0.0865 - val_accuracy: 0.9817\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0284 - accuracy: 0.9917 - val_loss: 0.0882 - val_accuracy: 0.9827\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0262 - accuracy: 0.9926 - val_loss: 0.0890 - val_accuracy: 0.9833\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.0967 - val_accuracy: 0.9826\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.1130 - val_accuracy: 0.9817\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.1165 - val_accuracy: 0.9831\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0216 - accuracy: 0.9938 - val_loss: 0.1122 - val_accuracy: 0.9827\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0180 - accuracy: 0.9948 - val_loss: 0.1239 - val_accuracy: 0.9824\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0185 - accuracy: 0.9950 - val_loss: 0.1193 - val_accuracy: 0.9829\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1213 - val_accuracy: 0.9832\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0178 - accuracy: 0.9953 - val_loss: 0.1210 - val_accuracy: 0.9826\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 0.1299 - val_accuracy: 0.9841\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "3         GlorotUniform   0.119634         0.9824\n",
      "4          GlorotNormal   0.129930         0.9844\n",
      "5              HeNormal   0.124419         0.9829\n",
      "6             HeUniform   0.137970         0.9821\n",
      "7           LecunNormal   0.129884         0.9841\n",
      "Model: \"sequential_478\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1297 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_836 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1298 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_837 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1299 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2510 - accuracy: 0.9226 - val_loss: 0.1186 - val_accuracy: 0.9627\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1046 - accuracy: 0.9687 - val_loss: 0.0812 - val_accuracy: 0.9762\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0755 - accuracy: 0.9779 - val_loss: 0.0809 - val_accuracy: 0.9765\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0598 - accuracy: 0.9819 - val_loss: 0.0761 - val_accuracy: 0.9785\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0510 - accuracy: 0.9850 - val_loss: 0.0912 - val_accuracy: 0.9762\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0444 - accuracy: 0.9869 - val_loss: 0.0829 - val_accuracy: 0.9803\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0380 - accuracy: 0.9890 - val_loss: 0.0822 - val_accuracy: 0.9806\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0333 - accuracy: 0.9899 - val_loss: 0.0873 - val_accuracy: 0.9814\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0321 - accuracy: 0.9904 - val_loss: 0.0834 - val_accuracy: 0.9827\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0294 - accuracy: 0.9915 - val_loss: 0.0951 - val_accuracy: 0.9826\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0260 - accuracy: 0.9921 - val_loss: 0.0919 - val_accuracy: 0.9830\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0250 - accuracy: 0.9930 - val_loss: 0.1176 - val_accuracy: 0.9784\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.1156 - val_accuracy: 0.9809\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0234 - accuracy: 0.9937 - val_loss: 0.1101 - val_accuracy: 0.9828\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0203 - accuracy: 0.9942 - val_loss: 0.1230 - val_accuracy: 0.9801\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 0.1108 - val_accuracy: 0.9829\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0190 - accuracy: 0.9948 - val_loss: 0.1043 - val_accuracy: 0.9834\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0169 - accuracy: 0.9953 - val_loss: 0.1119 - val_accuracy: 0.9823\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0199 - accuracy: 0.9947 - val_loss: 0.1359 - val_accuracy: 0.9830\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.1079 - val_accuracy: 0.9838\n",
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.301041         0.1135\n",
      "1          RandomNormal   0.114204         0.9831\n",
      "2         RandomUniform   0.110555         0.9845\n",
      "3         GlorotUniform   0.119634         0.9824\n",
      "4          GlorotNormal   0.129930         0.9844\n",
      "5              HeNormal   0.124419         0.9829\n",
      "6             HeUniform   0.137970         0.9821\n",
      "7           LecunNormal   0.129884         0.9841\n",
      "8          LecunUniform   0.107941         0.9838\n"
     ]
    }
   ],
   "source": [
    "# mnist_mlp.py + different initilization methods\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "seed_value = 42\n",
    "results = pd.DataFrame(columns=['Initialization Method', 'Test Loss', 'Test Accuracy'])\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "initilization_methods = [Zeros(), \n",
    "                       RandomNormal(seed=seed_value), \n",
    "                       RandomUniform(seed=seed_value), \n",
    "                       glorot_uniform(seed=seed_value), \n",
    "                       glorot_normal(seed=seed_value), \n",
    "                       he_normal(seed=seed_value), \n",
    "                       he_uniform(seed=seed_value), \n",
    "                       lecun_normal(seed=seed_value), \n",
    "                       lecun_uniform(seed=seed_value)]\n",
    "\n",
    "for method in initilization_methods:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,), kernel_initializer=method))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=method))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_initializer=method))\n",
    "\n",
    "    # Extract name of method\n",
    "    method_name = method.__class__.__name__\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    new_result = pd.DataFrame({'Initialization Method': [method_name], 'Test Loss': [score[0]], 'Test Accuracy': [score[1]]})\n",
    "    results = pd.concat([results, new_result], ignore_index=True)\n",
    "    \n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22f8cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_479\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1300 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_838 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1301 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_839 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1302 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2426 - accuracy: 0.9256 - val_loss: 0.1226 - val_accuracy: 0.9616\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1025 - accuracy: 0.9685 - val_loss: 0.0784 - val_accuracy: 0.9764\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0746 - accuracy: 0.9775 - val_loss: 0.0935 - val_accuracy: 0.9737\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0602 - accuracy: 0.9822 - val_loss: 0.0767 - val_accuracy: 0.9785\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0499 - accuracy: 0.9854 - val_loss: 0.0756 - val_accuracy: 0.9794\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0440 - accuracy: 0.9867 - val_loss: 0.0785 - val_accuracy: 0.9817\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0389 - accuracy: 0.9888 - val_loss: 0.0810 - val_accuracy: 0.9822\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0355 - accuracy: 0.9896 - val_loss: 0.0769 - val_accuracy: 0.9831\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0296 - accuracy: 0.9913 - val_loss: 0.0825 - val_accuracy: 0.9836\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0310 - accuracy: 0.9909 - val_loss: 0.0837 - val_accuracy: 0.9819\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.0884 - val_accuracy: 0.9836\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0244 - accuracy: 0.9925 - val_loss: 0.0951 - val_accuracy: 0.9828\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0241 - accuracy: 0.9934 - val_loss: 0.1012 - val_accuracy: 0.9826\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0222 - accuracy: 0.9934 - val_loss: 0.1032 - val_accuracy: 0.9821\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1064 - val_accuracy: 0.9835\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0203 - accuracy: 0.9946 - val_loss: 0.1151 - val_accuracy: 0.9836\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.1251 - val_accuracy: 0.9827\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0191 - accuracy: 0.9955 - val_loss: 0.1161 - val_accuracy: 0.9831\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0207 - accuracy: 0.9946 - val_loss: 0.1209 - val_accuracy: 0.9834\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.1380 - val_accuracy: 0.9828\n",
      "  Activation Method  Test Loss  Test Accuracy\n",
      "0              relu   0.137991         0.9828\n",
      "Model: \"sequential_480\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1303 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_840 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1304 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_841 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1305 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/1699284994.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 3s 7ms/step - loss: 0.5365 - accuracy: 0.8345 - val_loss: 0.2444 - val_accuracy: 0.9251\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2382 - accuracy: 0.9282 - val_loss: 0.1808 - val_accuracy: 0.9436\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1731 - accuracy: 0.9473 - val_loss: 0.1443 - val_accuracy: 0.9565\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1341 - accuracy: 0.9594 - val_loss: 0.1082 - val_accuracy: 0.9660\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1102 - accuracy: 0.9658 - val_loss: 0.0973 - val_accuracy: 0.9690\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0908 - accuracy: 0.9724 - val_loss: 0.0849 - val_accuracy: 0.9720\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0817 - accuracy: 0.9749 - val_loss: 0.0768 - val_accuracy: 0.9760\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0707 - accuracy: 0.9785 - val_loss: 0.0726 - val_accuracy: 0.9790\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0618 - accuracy: 0.9803 - val_loss: 0.0719 - val_accuracy: 0.9775\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0554 - accuracy: 0.9825 - val_loss: 0.0739 - val_accuracy: 0.9779\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0504 - accuracy: 0.9840 - val_loss: 0.0709 - val_accuracy: 0.9803\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0459 - accuracy: 0.9855 - val_loss: 0.0719 - val_accuracy: 0.9789\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0426 - accuracy: 0.9864 - val_loss: 0.0695 - val_accuracy: 0.9798\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0385 - accuracy: 0.9879 - val_loss: 0.0669 - val_accuracy: 0.9810\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0352 - accuracy: 0.9888 - val_loss: 0.0638 - val_accuracy: 0.9823\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0334 - accuracy: 0.9894 - val_loss: 0.0651 - val_accuracy: 0.9818\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0304 - accuracy: 0.9904 - val_loss: 0.0652 - val_accuracy: 0.9830\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 0.0671 - val_accuracy: 0.9825\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0283 - accuracy: 0.9911 - val_loss: 0.0688 - val_accuracy: 0.9822\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0256 - accuracy: 0.9918 - val_loss: 0.0653 - val_accuracy: 0.9829\n",
      "  Activation Method  Test Loss  Test Accuracy\n",
      "0              relu   0.137991         0.9828\n",
      "1           sigmoid   0.065342         0.9829\n",
      "Model: \"sequential_481\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1306 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_842 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1307 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_843 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1308 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3486 - accuracy: 0.8949 - val_loss: 0.1917 - val_accuracy: 0.9442\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1654 - accuracy: 0.9501 - val_loss: 0.1179 - val_accuracy: 0.9644\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1166 - accuracy: 0.9639 - val_loss: 0.1018 - val_accuracy: 0.9675\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0908 - accuracy: 0.9714 - val_loss: 0.0879 - val_accuracy: 0.9726\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0740 - accuracy: 0.9770 - val_loss: 0.0763 - val_accuracy: 0.9760\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0621 - accuracy: 0.9805 - val_loss: 0.0708 - val_accuracy: 0.9775\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0527 - accuracy: 0.9832 - val_loss: 0.0693 - val_accuracy: 0.9778\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 0.0703 - val_accuracy: 0.9781\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0397 - accuracy: 0.9872 - val_loss: 0.0646 - val_accuracy: 0.9812\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0361 - accuracy: 0.9886 - val_loss: 0.0639 - val_accuracy: 0.9816\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0312 - accuracy: 0.9896 - val_loss: 0.0667 - val_accuracy: 0.9794\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0288 - accuracy: 0.9902 - val_loss: 0.0706 - val_accuracy: 0.9784\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.0745 - val_accuracy: 0.9801\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0229 - accuracy: 0.9925 - val_loss: 0.0660 - val_accuracy: 0.9811\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.0723 - val_accuracy: 0.9798\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0658 - val_accuracy: 0.9811\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0175 - accuracy: 0.9940 - val_loss: 0.0705 - val_accuracy: 0.9795\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.0707 - val_accuracy: 0.9818\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 0.0674 - val_accuracy: 0.9818\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.0654 - val_accuracy: 0.9820\n",
      "  Activation Method  Test Loss  Test Accuracy\n",
      "0              relu   0.137991         0.9828\n",
      "1           sigmoid   0.065342         0.9829\n",
      "2              tanh   0.065353         0.9820\n",
      "Model: \"sequential_482\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1309 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_844 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1310 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_845 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1311 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.4614 - accuracy: 0.8665 - val_loss: 0.5015 - val_accuracy: 0.8595\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3763 - accuracy: 0.8940 - val_loss: 0.3259 - val_accuracy: 0.9088\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3522 - accuracy: 0.9005 - val_loss: 0.3298 - val_accuracy: 0.9048\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3402 - accuracy: 0.9038 - val_loss: 0.3320 - val_accuracy: 0.9033\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3317 - accuracy: 0.9079 - val_loss: 0.3092 - val_accuracy: 0.9138\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3269 - accuracy: 0.9089 - val_loss: 0.3209 - val_accuracy: 0.9053\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3240 - accuracy: 0.9086 - val_loss: 0.3586 - val_accuracy: 0.8932\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3177 - accuracy: 0.9094 - val_loss: 0.3248 - val_accuracy: 0.9105\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3177 - accuracy: 0.9117 - val_loss: 0.3129 - val_accuracy: 0.9180\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3159 - accuracy: 0.9113 - val_loss: 0.3042 - val_accuracy: 0.9145\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3119 - accuracy: 0.9125 - val_loss: 0.3214 - val_accuracy: 0.9078\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3127 - accuracy: 0.9123 - val_loss: 0.3139 - val_accuracy: 0.9139\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3113 - accuracy: 0.9132 - val_loss: 0.3252 - val_accuracy: 0.9105\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3094 - accuracy: 0.9140 - val_loss: 0.3552 - val_accuracy: 0.8958\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3076 - accuracy: 0.9135 - val_loss: 0.2922 - val_accuracy: 0.9184\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3067 - accuracy: 0.9141 - val_loss: 0.3375 - val_accuracy: 0.9077\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3058 - accuracy: 0.9146 - val_loss: 0.3104 - val_accuracy: 0.9172\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3056 - accuracy: 0.9134 - val_loss: 0.3081 - val_accuracy: 0.9147\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3058 - accuracy: 0.9147 - val_loss: 0.3058 - val_accuracy: 0.9188\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3057 - accuracy: 0.9148 - val_loss: 0.3224 - val_accuracy: 0.9101\n",
      "  Activation Method  Test Loss  Test Accuracy\n",
      "0              relu   0.137991         0.9828\n",
      "1           sigmoid   0.065342         0.9829\n",
      "2              tanh   0.065353         0.9820\n",
      "3            linear   0.322379         0.9101\n",
      "Model: \"sequential_483\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1312 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_846 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1313 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_847 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1314 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.2815 - accuracy: 0.1465 - val_loss: 2.2280 - val_accuracy: 0.2944\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 2.1132 - accuracy: 0.4152 - val_loss: 1.9473 - val_accuracy: 0.4882\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.8353 - accuracy: 0.4167 - val_loss: 1.6077 - val_accuracy: 0.4882\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5923 - accuracy: 0.4157 - val_loss: 1.3424 - val_accuracy: 0.4889\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4288 - accuracy: 0.4220 - val_loss: 1.1844 - val_accuracy: 0.4887\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.3354 - accuracy: 0.4290 - val_loss: 1.0972 - val_accuracy: 0.4935\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.2922 - accuracy: 0.4357 - val_loss: 1.0548 - val_accuracy: 0.4979\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.2597 - accuracy: 0.4445 - val_loss: 1.0323 - val_accuracy: 0.5035\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.2398 - accuracy: 0.4490 - val_loss: 1.0158 - val_accuracy: 0.5083\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.2263 - accuracy: 0.4554 - val_loss: 1.0030 - val_accuracy: 0.5171\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.2072 - accuracy: 0.4708 - val_loss: 0.9891 - val_accuracy: 0.5385\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.1859 - accuracy: 0.4956 - val_loss: 0.9760 - val_accuracy: 0.5732\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.1467 - accuracy: 0.5472 - val_loss: 0.9259 - val_accuracy: 0.6878\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.0778 - accuracy: 0.5969 - val_loss: 0.8524 - val_accuracy: 0.7252\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.0058 - accuracy: 0.6315 - val_loss: 0.7736 - val_accuracy: 0.7824\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.9451 - accuracy: 0.6660 - val_loss: 0.7116 - val_accuracy: 0.8175\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8954 - accuracy: 0.6919 - val_loss: 0.6635 - val_accuracy: 0.8399\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.8504 - accuracy: 0.7203 - val_loss: 0.6106 - val_accuracy: 0.8578\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7972 - accuracy: 0.7464 - val_loss: 0.5613 - val_accuracy: 0.8742\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.7575 - accuracy: 0.7560 - val_loss: 0.5072 - val_accuracy: 0.8923\n",
      "  Activation Method  Test Loss  Test Accuracy\n",
      "0              relu   0.137991         0.9828\n",
      "1           sigmoid   0.065342         0.9829\n",
      "2              tanh   0.065353         0.9820\n",
      "3            linear   0.322379         0.9101\n",
      "4           softmax   0.507233         0.8923\n"
     ]
    }
   ],
   "source": [
    "# mnist_mlp.py + different activation functions\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "seed_value = 42\n",
    "results = pd.DataFrame(columns=['Activation Method', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "activation_methods = ['relu', 'sigmoid', 'tanh', 'linear', 'softmax']\n",
    "\n",
    "\n",
    "\n",
    "for method in activation_methods:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation=method, input_shape=(784,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation=method))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    \n",
    "    method_name = method\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    new_result = pd.DataFrame({'Activation Method': [method_name], 'Test Loss': [score[0]], 'Test Accuracy': [score[1]]})\n",
    "    results = pd.concat([results, new_result], ignore_index=True)\n",
    "    \n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab475863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_484\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1315 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_848 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1316 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_849 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1317 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2482 - accuracy: 0.9257 - val_loss: 0.1002 - val_accuracy: 0.9688\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0979 - accuracy: 0.9700 - val_loss: 0.0718 - val_accuracy: 0.9787\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0690 - accuracy: 0.9784 - val_loss: 0.0677 - val_accuracy: 0.9785\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0550 - accuracy: 0.9823 - val_loss: 0.0668 - val_accuracy: 0.9800\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0459 - accuracy: 0.9850 - val_loss: 0.0705 - val_accuracy: 0.9779\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0380 - accuracy: 0.9881 - val_loss: 0.0771 - val_accuracy: 0.9778\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0357 - accuracy: 0.9882 - val_loss: 0.0583 - val_accuracy: 0.9826\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0313 - accuracy: 0.9896 - val_loss: 0.0714 - val_accuracy: 0.9794\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.0679 - val_accuracy: 0.9813\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0655 - val_accuracy: 0.9826\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0220 - accuracy: 0.9930 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0236 - accuracy: 0.9920 - val_loss: 0.0673 - val_accuracy: 0.9821\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0216 - accuracy: 0.9929 - val_loss: 0.0748 - val_accuracy: 0.9804\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0703 - val_accuracy: 0.9826\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0191 - accuracy: 0.9937 - val_loss: 0.0773 - val_accuracy: 0.9836\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.0685 - val_accuracy: 0.9831\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0188 - accuracy: 0.9934 - val_loss: 0.0831 - val_accuracy: 0.9834\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0905 - val_accuracy: 0.9829\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.0684 - val_accuracy: 0.9849\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0156 - accuracy: 0.9953 - val_loss: 0.0762 - val_accuracy: 0.9843\n",
      "  Optimizer  Test Loss  Test Accuracy\n",
      "0      Adam   0.076241         0.9843\n",
      "Model: \"sequential_485\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1318 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_850 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1319 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_851 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1320 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "  1/469 [..............................] - ETA: 1:00 - loss: 2.3285 - accuracy: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/1283481495.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 4ms/step - loss: 1.2323 - accuracy: 0.6775 - val_loss: 0.5515 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.5400 - accuracy: 0.8483 - val_loss: 0.3795 - val_accuracy: 0.8989\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.4289 - accuracy: 0.8767 - val_loss: 0.3252 - val_accuracy: 0.9080\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.3795 - accuracy: 0.8907 - val_loss: 0.2942 - val_accuracy: 0.9179\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.3457 - accuracy: 0.8991 - val_loss: 0.2725 - val_accuracy: 0.9227\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3192 - accuracy: 0.9066 - val_loss: 0.2545 - val_accuracy: 0.9287\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2981 - accuracy: 0.9133 - val_loss: 0.2395 - val_accuracy: 0.9309\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2820 - accuracy: 0.9177 - val_loss: 0.2272 - val_accuracy: 0.9339\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2672 - accuracy: 0.9225 - val_loss: 0.2179 - val_accuracy: 0.9369\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2542 - accuracy: 0.9268 - val_loss: 0.2076 - val_accuracy: 0.9393\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2433 - accuracy: 0.9301 - val_loss: 0.1978 - val_accuracy: 0.9417\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2333 - accuracy: 0.9329 - val_loss: 0.1910 - val_accuracy: 0.9437\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2233 - accuracy: 0.9350 - val_loss: 0.1831 - val_accuracy: 0.9458\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2151 - accuracy: 0.9382 - val_loss: 0.1772 - val_accuracy: 0.9479\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2069 - accuracy: 0.9401 - val_loss: 0.1704 - val_accuracy: 0.9491\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2002 - accuracy: 0.9419 - val_loss: 0.1646 - val_accuracy: 0.9507\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1921 - accuracy: 0.9446 - val_loss: 0.1597 - val_accuracy: 0.9527\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9452 - val_loss: 0.1550 - val_accuracy: 0.9527\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1799 - accuracy: 0.9478 - val_loss: 0.1502 - val_accuracy: 0.9543\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1751 - accuracy: 0.9491 - val_loss: 0.1454 - val_accuracy: 0.9550\n",
      "  Optimizer  Test Loss  Test Accuracy\n",
      "0      Adam   0.076241         0.9843\n",
      "1       SGD   0.145389         0.9550\n",
      "Model: \"sequential_486\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1321 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_852 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1322 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_853 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1323 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2455 - accuracy: 0.9254 - val_loss: 0.1147 - val_accuracy: 0.9615\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1020 - accuracy: 0.9689 - val_loss: 0.0903 - val_accuracy: 0.9719\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0735 - accuracy: 0.9773 - val_loss: 0.0846 - val_accuracy: 0.9763\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0591 - accuracy: 0.9818 - val_loss: 0.0840 - val_accuracy: 0.9774\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0508 - accuracy: 0.9846 - val_loss: 0.0764 - val_accuracy: 0.9800\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0421 - accuracy: 0.9872 - val_loss: 0.0719 - val_accuracy: 0.9807\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0379 - accuracy: 0.9887 - val_loss: 0.0878 - val_accuracy: 0.9798\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0338 - accuracy: 0.9900 - val_loss: 0.0860 - val_accuracy: 0.9811\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0329 - accuracy: 0.9909 - val_loss: 0.0822 - val_accuracy: 0.9830\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.0967 - val_accuracy: 0.9811\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0271 - accuracy: 0.9921 - val_loss: 0.0952 - val_accuracy: 0.9817\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0252 - accuracy: 0.9930 - val_loss: 0.1063 - val_accuracy: 0.9813\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0227 - accuracy: 0.9935 - val_loss: 0.1089 - val_accuracy: 0.9829\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.1156 - val_accuracy: 0.9803\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1141 - val_accuracy: 0.9808\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0208 - accuracy: 0.9946 - val_loss: 0.1109 - val_accuracy: 0.9833\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0187 - accuracy: 0.9949 - val_loss: 0.1267 - val_accuracy: 0.9814\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 0.1172 - val_accuracy: 0.9832\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0180 - accuracy: 0.9950 - val_loss: 0.1370 - val_accuracy: 0.9834\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1173 - val_accuracy: 0.9837\n",
      "  Optimizer  Test Loss  Test Accuracy\n",
      "0      Adam   0.076241         0.9843\n",
      "1       SGD   0.145389         0.9550\n",
      "2   RMSprop   0.117272         0.9837\n"
     ]
    }
   ],
   "source": [
    "# mnist_mlp.py + different optimizers\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "seed_value = 42\n",
    "results = pd.DataFrame(columns=['Optimizer', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Use the optimizer's name as method_name\n",
    "    method_name = optimizer.__name__\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    new_result = pd.DataFrame({'Optimizer': [method_name], 'Test Loss': [score[0]], 'Test Accuracy': [score[1]]})\n",
    "    results = pd.concat([results, new_result], ignore_index=True)\n",
    "    \n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1db0c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_487\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1324 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1325 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2567 - accuracy: 0.9258 - val_loss: 0.1438 - val_accuracy: 0.9552\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1045 - accuracy: 0.9689 - val_loss: 0.0858 - val_accuracy: 0.9729\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0689 - accuracy: 0.9797 - val_loss: 0.0818 - val_accuracy: 0.9740\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0501 - accuracy: 0.9844 - val_loss: 0.0786 - val_accuracy: 0.9756\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9889 - val_loss: 0.0698 - val_accuracy: 0.9797\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0278 - accuracy: 0.9918 - val_loss: 0.0668 - val_accuracy: 0.9803\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.0657 - val_accuracy: 0.9814\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0750 - val_accuracy: 0.9784\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.0697 - val_accuracy: 0.9819\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 0.0725 - val_accuracy: 0.9810\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0743 - val_accuracy: 0.9815\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.0773 - val_accuracy: 0.9805\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0753 - val_accuracy: 0.9835\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0766 - val_accuracy: 0.9822\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0807 - val_accuracy: 0.9821\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0936 - val_accuracy: 0.9811\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0939 - val_accuracy: 0.9804\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0882 - val_accuracy: 0.9815\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0912 - val_accuracy: 0.9818\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 9.0687e-04 - accuracy: 0.9997 - val_loss: 0.0969 - val_accuracy: 0.9828\n",
      "  Regularization Method  Test Loss  Test Accuracy\n",
      "0                  None   0.096896         0.9828\n",
      "Model: \"sequential_488\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1326 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1327 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1328 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/400663256.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 3ms/step - loss: 6.5283 - accuracy: 0.8138 - val_loss: 1.9965 - val_accuracy: 0.8889\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.8611 - accuracy: 0.9043 - val_loss: 1.7738 - val_accuracy: 0.9239\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.7305 - accuracy: 0.9251 - val_loss: 1.7085 - val_accuracy: 0.9304\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.6662 - accuracy: 0.9360 - val_loss: 1.6318 - val_accuracy: 0.9433\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.6267 - accuracy: 0.9433 - val_loss: 1.6111 - val_accuracy: 0.9471\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.5968 - accuracy: 0.9488 - val_loss: 1.6235 - val_accuracy: 0.9389\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.5733 - accuracy: 0.9537 - val_loss: 1.5847 - val_accuracy: 0.9437\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.5548 - accuracy: 0.9572 - val_loss: 1.5506 - val_accuracy: 0.9556\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5392 - accuracy: 0.9606 - val_loss: 1.5437 - val_accuracy: 0.9573\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5273 - accuracy: 0.9627 - val_loss: 1.5229 - val_accuracy: 0.9631\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.5159 - accuracy: 0.9646 - val_loss: 1.5072 - val_accuracy: 0.9624\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.5062 - accuracy: 0.9674 - val_loss: 1.5100 - val_accuracy: 0.9650\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.4978 - accuracy: 0.9689 - val_loss: 1.4988 - val_accuracy: 0.9658\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4905 - accuracy: 0.9703 - val_loss: 1.4894 - val_accuracy: 0.9674\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.4838 - accuracy: 0.9715 - val_loss: 1.4999 - val_accuracy: 0.9650\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.4789 - accuracy: 0.9721 - val_loss: 1.4789 - val_accuracy: 0.9677\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 1.4737 - accuracy: 0.9736 - val_loss: 1.4920 - val_accuracy: 0.9666\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4696 - accuracy: 0.9735 - val_loss: 1.4776 - val_accuracy: 0.9697\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4653 - accuracy: 0.9749 - val_loss: 1.4752 - val_accuracy: 0.9706\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 1.4615 - accuracy: 0.9755 - val_loss: 1.4667 - val_accuracy: 0.9718\n",
      "  Regularization Method  Test Loss  Test Accuracy\n",
      "0                  None   0.096896         0.9828\n",
      "1                    L1   1.466660         0.9718\n",
      "Model: \"sequential_489\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1329 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_1330 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1331 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.7449 - accuracy: 0.9114 - val_loss: 0.3142 - val_accuracy: 0.9281\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2109 - accuracy: 0.9576 - val_loss: 0.1735 - val_accuracy: 0.9664\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1639 - accuracy: 0.9674 - val_loss: 0.1725 - val_accuracy: 0.9653\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1416 - accuracy: 0.9726 - val_loss: 0.1477 - val_accuracy: 0.9715\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1261 - accuracy: 0.9761 - val_loss: 0.1452 - val_accuracy: 0.9706\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1152 - accuracy: 0.9791 - val_loss: 0.1323 - val_accuracy: 0.9693\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1058 - accuracy: 0.9810 - val_loss: 0.1286 - val_accuracy: 0.9740\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.1005 - accuracy: 0.9818 - val_loss: 0.1449 - val_accuracy: 0.9728\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0941 - accuracy: 0.9826 - val_loss: 0.1178 - val_accuracy: 0.9779\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0879 - accuracy: 0.9848 - val_loss: 0.1346 - val_accuracy: 0.9745\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0844 - accuracy: 0.9858 - val_loss: 0.1433 - val_accuracy: 0.9697\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0808 - accuracy: 0.9864 - val_loss: 0.1137 - val_accuracy: 0.9768\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9870 - val_loss: 0.1208 - val_accuracy: 0.9762\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0740 - accuracy: 0.9879 - val_loss: 0.1170 - val_accuracy: 0.9786\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0713 - accuracy: 0.9883 - val_loss: 0.1214 - val_accuracy: 0.9763\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0683 - accuracy: 0.9890 - val_loss: 0.1144 - val_accuracy: 0.9768\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0653 - accuracy: 0.9898 - val_loss: 0.1172 - val_accuracy: 0.9767\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0628 - accuracy: 0.9901 - val_loss: 0.1423 - val_accuracy: 0.9708\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0610 - accuracy: 0.9903 - val_loss: 0.1246 - val_accuracy: 0.9785\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0596 - accuracy: 0.9906 - val_loss: 0.1250 - val_accuracy: 0.9763\n",
      "  Regularization Method  Test Loss  Test Accuracy\n",
      "0                  None   0.096896         0.9828\n",
      "1                    L1   1.466660         0.9718\n",
      "2                    L2   0.124994         0.9763\n",
      "Model: \"sequential_490\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1332 (Dense)          (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout_854 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1333 (Dense)          (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_855 (Dropout)       (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1334 (Dense)          (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2485 - accuracy: 0.9218 - val_loss: 0.0970 - val_accuracy: 0.9692\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1022 - accuracy: 0.9683 - val_loss: 0.0903 - val_accuracy: 0.9728\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0742 - accuracy: 0.9769 - val_loss: 0.0870 - val_accuracy: 0.9737\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0604 - accuracy: 0.9821 - val_loss: 0.0663 - val_accuracy: 0.9819\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0503 - accuracy: 0.9845 - val_loss: 0.0873 - val_accuracy: 0.9775\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0427 - accuracy: 0.9873 - val_loss: 0.0769 - val_accuracy: 0.9808\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.0734 - val_accuracy: 0.9830\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0359 - accuracy: 0.9892 - val_loss: 0.0679 - val_accuracy: 0.9839\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0324 - accuracy: 0.9907 - val_loss: 0.0828 - val_accuracy: 0.9819\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0278 - accuracy: 0.9915 - val_loss: 0.0808 - val_accuracy: 0.9847\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0257 - accuracy: 0.9926 - val_loss: 0.0978 - val_accuracy: 0.9805\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0246 - accuracy: 0.9934 - val_loss: 0.0941 - val_accuracy: 0.9823\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0241 - accuracy: 0.9929 - val_loss: 0.0892 - val_accuracy: 0.9830\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0217 - accuracy: 0.9937 - val_loss: 0.0980 - val_accuracy: 0.9823\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0191 - accuracy: 0.9944 - val_loss: 0.1074 - val_accuracy: 0.9827\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0197 - accuracy: 0.9947 - val_loss: 0.1018 - val_accuracy: 0.9845\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 0.1426 - val_accuracy: 0.9803\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0212 - accuracy: 0.9946 - val_loss: 0.1214 - val_accuracy: 0.9822\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0188 - accuracy: 0.9952 - val_loss: 0.1378 - val_accuracy: 0.9823\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0184 - accuracy: 0.9952 - val_loss: 0.1304 - val_accuracy: 0.9843\n",
      "  Regularization Method  Test Loss  Test Accuracy\n",
      "0                  None   0.096896         0.9828\n",
      "1                    L1   1.466660         0.9718\n",
      "2                    L2   0.124994         0.9763\n",
      "3               Dropout   0.130384         0.9843\n"
     ]
    }
   ],
   "source": [
    "# mnist_mlp.py + different regularization techniques\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape (10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "seed_value = 42\n",
    "results = pd.DataFrame(columns=['Regularization Method', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "regularization_methods = ['None', 'L1', 'L2', 'Dropout']\n",
    "\n",
    "for method in regularization_methods:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "\n",
    "    if method == 'L1':\n",
    "        model.add(Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "    elif method == 'L2':\n",
    "        model.add(Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "    elif method == 'Dropout':\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Use the regularization method as method_name\n",
    "    method_name = method\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    new_result = pd.DataFrame({'Regularization Method': [method_name], 'Test Loss': [score[0]], 'Test Accuracy': [score[1]]})\n",
    "    results = pd.concat([results, new_result], ignore_index=True)\n",
    "\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03cf34af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/1166043142.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Best Performing Configurations:\n",
      "   Optimizer  Learning Rate Momentum       Epsilon Nesterov Batch Size  \\\n",
      "52       SGD            0.1        0  1.000000e-04    False         64   \n",
      "60       SGD            0.1      0.9  1.000000e-04    False         64   \n",
      "48       SGD            0.1        0  1.000000e-08    False         64   \n",
      "54       SGD            0.1        0  1.000000e-04     True         64   \n",
      "50       SGD            0.1        0  1.000000e-08     True         64   \n",
      "\n",
      "    Test Loss  Test Accuracy  \n",
      "52   0.065562         0.9795  \n",
      "60   0.068581         0.9783  \n",
      "48   0.073224         0.9772  \n",
      "54   0.072099         0.9768  \n",
      "50   0.072205         0.9768  \n",
      "   Optimizer  Learning Rate Momentum       Epsilon Nesterov Batch Size  \\\n",
      "0       Adam        0.00001        0  1.000000e-08    False         64   \n",
      "1       Adam        0.00001        0  1.000000e-08    False        256   \n",
      "2       Adam        0.00001        0  1.000000e-08     True         64   \n",
      "3       Adam        0.00001        0  1.000000e-08     True        256   \n",
      "4       Adam        0.00001        0  1.000000e-04    False         64   \n",
      "5       Adam        0.00001        0  1.000000e-04    False        256   \n",
      "6       Adam        0.00001        0  1.000000e-04     True         64   \n",
      "7       Adam        0.00001        0  1.000000e-04     True        256   \n",
      "8       Adam        0.00001      0.9  1.000000e-08    False         64   \n",
      "9       Adam        0.00001      0.9  1.000000e-08    False        256   \n",
      "10      Adam        0.00001      0.9  1.000000e-08     True         64   \n",
      "11      Adam        0.00001      0.9  1.000000e-08     True        256   \n",
      "12      Adam        0.00001      0.9  1.000000e-04    False         64   \n",
      "13      Adam        0.00001      0.9  1.000000e-04    False        256   \n",
      "14      Adam        0.00001      0.9  1.000000e-04     True         64   \n",
      "15      Adam        0.00001      0.9  1.000000e-04     True        256   \n",
      "16      Adam        0.10000        0  1.000000e-08    False         64   \n",
      "17      Adam        0.10000        0  1.000000e-08    False        256   \n",
      "18      Adam        0.10000        0  1.000000e-08     True         64   \n",
      "19      Adam        0.10000        0  1.000000e-08     True        256   \n",
      "20      Adam        0.10000        0  1.000000e-04    False         64   \n",
      "21      Adam        0.10000        0  1.000000e-04    False        256   \n",
      "22      Adam        0.10000        0  1.000000e-04     True         64   \n",
      "23      Adam        0.10000        0  1.000000e-04     True        256   \n",
      "24      Adam        0.10000      0.9  1.000000e-08    False         64   \n",
      "25      Adam        0.10000      0.9  1.000000e-08    False        256   \n",
      "26      Adam        0.10000      0.9  1.000000e-08     True         64   \n",
      "27      Adam        0.10000      0.9  1.000000e-08     True        256   \n",
      "28      Adam        0.10000      0.9  1.000000e-04    False         64   \n",
      "29      Adam        0.10000      0.9  1.000000e-04    False        256   \n",
      "30      Adam        0.10000      0.9  1.000000e-04     True         64   \n",
      "31      Adam        0.10000      0.9  1.000000e-04     True        256   \n",
      "32       SGD        0.00001        0  1.000000e-08    False         64   \n",
      "33       SGD        0.00001        0  1.000000e-08    False        256   \n",
      "34       SGD        0.00001        0  1.000000e-08     True         64   \n",
      "35       SGD        0.00001        0  1.000000e-08     True        256   \n",
      "36       SGD        0.00001        0  1.000000e-04    False         64   \n",
      "37       SGD        0.00001        0  1.000000e-04    False        256   \n",
      "38       SGD        0.00001        0  1.000000e-04     True         64   \n",
      "39       SGD        0.00001        0  1.000000e-04     True        256   \n",
      "40       SGD        0.00001      0.9  1.000000e-08    False         64   \n",
      "41       SGD        0.00001      0.9  1.000000e-08    False        256   \n",
      "42       SGD        0.00001      0.9  1.000000e-08     True         64   \n",
      "43       SGD        0.00001      0.9  1.000000e-08     True        256   \n",
      "44       SGD        0.00001      0.9  1.000000e-04    False         64   \n",
      "45       SGD        0.00001      0.9  1.000000e-04    False        256   \n",
      "46       SGD        0.00001      0.9  1.000000e-04     True         64   \n",
      "47       SGD        0.00001      0.9  1.000000e-04     True        256   \n",
      "48       SGD        0.10000        0  1.000000e-08    False         64   \n",
      "49       SGD        0.10000        0  1.000000e-08    False        256   \n",
      "50       SGD        0.10000        0  1.000000e-08     True         64   \n",
      "51       SGD        0.10000        0  1.000000e-08     True        256   \n",
      "52       SGD        0.10000        0  1.000000e-04    False         64   \n",
      "53       SGD        0.10000        0  1.000000e-04    False        256   \n",
      "54       SGD        0.10000        0  1.000000e-04     True         64   \n",
      "55       SGD        0.10000        0  1.000000e-04     True        256   \n",
      "56       SGD        0.10000      0.9  1.000000e-08    False         64   \n",
      "57       SGD        0.10000      0.9  1.000000e-08    False        256   \n",
      "58       SGD        0.10000      0.9  1.000000e-08     True         64   \n",
      "59       SGD        0.10000      0.9  1.000000e-08     True        256   \n",
      "60       SGD        0.10000      0.9  1.000000e-04    False         64   \n",
      "61       SGD        0.10000      0.9  1.000000e-04    False        256   \n",
      "62       SGD        0.10000      0.9  1.000000e-04     True         64   \n",
      "63       SGD        0.10000      0.9  1.000000e-04     True        256   \n",
      "64   RMSprop        0.00001        0  1.000000e-08    False         64   \n",
      "65   RMSprop        0.00001        0  1.000000e-08    False        256   \n",
      "66   RMSprop        0.00001        0  1.000000e-08     True         64   \n",
      "67   RMSprop        0.00001        0  1.000000e-08     True        256   \n",
      "68   RMSprop        0.00001        0  1.000000e-04    False         64   \n",
      "69   RMSprop        0.00001        0  1.000000e-04    False        256   \n",
      "70   RMSprop        0.00001        0  1.000000e-04     True         64   \n",
      "71   RMSprop        0.00001        0  1.000000e-04     True        256   \n",
      "72   RMSprop        0.00001      0.9  1.000000e-08    False         64   \n",
      "73   RMSprop        0.00001      0.9  1.000000e-08    False        256   \n",
      "74   RMSprop        0.00001      0.9  1.000000e-08     True         64   \n",
      "75   RMSprop        0.00001      0.9  1.000000e-08     True        256   \n",
      "76   RMSprop        0.00001      0.9  1.000000e-04    False         64   \n",
      "77   RMSprop        0.00001      0.9  1.000000e-04    False        256   \n",
      "78   RMSprop        0.00001      0.9  1.000000e-04     True         64   \n",
      "79   RMSprop        0.00001      0.9  1.000000e-04     True        256   \n",
      "80   RMSprop        0.10000        0  1.000000e-08    False         64   \n",
      "81   RMSprop        0.10000        0  1.000000e-08    False        256   \n",
      "82   RMSprop        0.10000        0  1.000000e-08     True         64   \n",
      "83   RMSprop        0.10000        0  1.000000e-08     True        256   \n",
      "84   RMSprop        0.10000        0  1.000000e-04    False         64   \n",
      "85   RMSprop        0.10000        0  1.000000e-04    False        256   \n",
      "86   RMSprop        0.10000        0  1.000000e-04     True         64   \n",
      "87   RMSprop        0.10000        0  1.000000e-04     True        256   \n",
      "88   RMSprop        0.10000      0.9  1.000000e-08    False         64   \n",
      "89   RMSprop        0.10000      0.9  1.000000e-08    False        256   \n",
      "90   RMSprop        0.10000      0.9  1.000000e-08     True         64   \n",
      "91   RMSprop        0.10000      0.9  1.000000e-08     True        256   \n",
      "92   RMSprop        0.10000      0.9  1.000000e-04    False         64   \n",
      "93   RMSprop        0.10000      0.9  1.000000e-04    False        256   \n",
      "94   RMSprop        0.10000      0.9  1.000000e-04     True         64   \n",
      "95   RMSprop        0.10000      0.9  1.000000e-04     True        256   \n",
      "\n",
      "    Test Loss  Test Accuracy  \n",
      "0    0.276135         0.9234  \n",
      "1    0.440184         0.8936  \n",
      "2    0.279541         0.9204  \n",
      "3    0.428770         0.8930  \n",
      "4    0.278455         0.9210  \n",
      "5    0.433539         0.8897  \n",
      "6    0.273551         0.9219  \n",
      "7    0.441681         0.8940  \n",
      "8    0.275638         0.9232  \n",
      "9    0.436570         0.8917  \n",
      "10   0.275248         0.9210  \n",
      "11   0.431542         0.8958  \n",
      "12   0.279481         0.9214  \n",
      "13   0.428507         0.8928  \n",
      "14   0.274365         0.9227  \n",
      "15   0.427427         0.8951  \n",
      "16   2.312618         0.1028  \n",
      "17   2.643875         0.1407  \n",
      "18   2.034741         0.2090  \n",
      "19   2.006779         0.2206  \n",
      "20   2.308732         0.0958  \n",
      "21   1.356306         0.4514  \n",
      "22   2.309549         0.0980  \n",
      "23   1.505841         0.4317  \n",
      "24   2.314147         0.0974  \n",
      "25   1.482209         0.3807  \n",
      "26   2.310136         0.0980  \n",
      "27   1.379112         0.4717  \n",
      "28   2.315363         0.1010  \n",
      "29   1.204400         0.5516  \n",
      "30   2.309433         0.0979  \n",
      "31   1.528986         0.3847  \n",
      "32   2.307562         0.1090  \n",
      "33   2.329525         0.1163  \n",
      "34   2.268434         0.1392  \n",
      "35   2.356221         0.1080  \n",
      "36   2.281428         0.1247  \n",
      "37   2.321924         0.0830  \n",
      "38   2.271350         0.1336  \n",
      "39   2.342400         0.0918  \n",
      "40   2.300761         0.1032  \n",
      "41   2.378359         0.0646  \n",
      "42   2.248410         0.1930  \n",
      "43   2.319686         0.1063  \n",
      "44   2.267238         0.1437  \n",
      "45   2.321155         0.1066  \n",
      "46   2.321980         0.0800  \n",
      "47   2.342832         0.1079  \n",
      "48   0.073224         0.9772  \n",
      "49   0.130684         0.9606  \n",
      "50   0.072205         0.9768  \n",
      "51   0.139153         0.9572  \n",
      "52   0.065562         0.9795  \n",
      "53   0.134951         0.9601  \n",
      "54   0.072099         0.9768  \n",
      "55   0.137186         0.9585  \n",
      "56   0.071208         0.9766  \n",
      "57   0.132321         0.9589  \n",
      "58   0.077412         0.9762  \n",
      "59   0.137868         0.9572  \n",
      "60   0.068581         0.9783  \n",
      "61   0.134241         0.9596  \n",
      "62   0.075777         0.9761  \n",
      "63   0.130876         0.9610  \n",
      "64   0.281479         0.9223  \n",
      "65   0.442392         0.8911  \n",
      "66   0.281502         0.9190  \n",
      "67   0.432365         0.8909  \n",
      "68   0.293928         0.9189  \n",
      "69   0.482716         0.8854  \n",
      "70   0.289875         0.9193  \n",
      "71   0.469030         0.8880  \n",
      "72   0.278721         0.9219  \n",
      "73   0.435601         0.8926  \n",
      "74   0.280181         0.9210  \n",
      "75   0.437292         0.8899  \n",
      "76   0.291479         0.9175  \n",
      "77   0.466193         0.8857  \n",
      "78   0.288631         0.9197  \n",
      "79   0.477542         0.8870  \n",
      "80   1.675124         0.3758  \n",
      "81   1.692917         0.3568  \n",
      "82   1.983952         0.2852  \n",
      "83   1.739162         0.3251  \n",
      "84   1.658895         0.3091  \n",
      "85   1.626555         0.3863  \n",
      "86   1.737189         0.3780  \n",
      "87   2.085721         0.2929  \n",
      "88   1.822181         0.2816  \n",
      "89   1.485665         0.4440  \n",
      "90   1.789143         0.3024  \n",
      "91   1.545071         0.4686  \n",
      "92   1.606072         0.4606  \n",
      "93   1.602309         0.3851  \n",
      "94   2.184098         0.1791  \n",
      "95   1.518015         0.4244  \n"
     ]
    }
   ],
   "source": [
    "# mnist_mlp.py + different regularization techniques\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Define hyperparameter values to test\n",
    "learning_rates = [0.00001, 0.1]\n",
    "momentum_values = [0, 0.9]\n",
    "epsilon_values = [1e-8, 1e-4]\n",
    "nesterov_values = [False, True]\n",
    "batch_sizes = [64, 256]\n",
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Optimizer', 'Learning Rate', 'Momentum', 'Epsilon', 'Nesterov', 'Batch Size', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Test different hyperparameters\n",
    "for optimizer in optimizers:\n",
    "    for lr in learning_rates:\n",
    "        for momentum in momentum_values:\n",
    "            for epsilon in epsilon_values:\n",
    "                for nesterov in nesterov_values:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        model = Sequential()\n",
    "                        model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "                        model.add(Dropout(0.2))\n",
    "                        model.add(Dense(512, activation='relu'))\n",
    "                        model.add(Dropout(0.2))\n",
    "                        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "                        optimizer_name = optimizer.__name__\n",
    "                        optimizer_instance = optimizer(learning_rate=lr)\n",
    "\n",
    "                        if optimizer_name == 'SGD':\n",
    "                            optimizer_instance.momentum = momentum\n",
    "                            optimizer_instance.nesterov = nesterov\n",
    "                        elif optimizer_name == 'RMSprop':\n",
    "                            optimizer_instance.epsilon = epsilon\n",
    "\n",
    "                        model.compile(loss='categorical_crossentropy',\n",
    "                                      optimizer=optimizer_instance,\n",
    "                                      metrics=['accuracy'])\n",
    "\n",
    "                        history = model.fit(x_train, y_train, batch_size=batch_size, epochs=5, verbose=0, validation_data=(x_test, y_test))\n",
    "                        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "                        new_result = pd.DataFrame({'Optimizer': [optimizer_name], 'Learning Rate': [lr], 'Momentum': [momentum], 'Epsilon': [epsilon], 'Nesterov': [nesterov], 'Batch Size': [batch_size], 'Test Loss': [score[0]], 'Test Accuracy': [score[1]]})\n",
    "                        results = pd.concat([results, new_result], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Display the results\n",
    "pd.set_option('display.max_rows', None)  # To display all rows\n",
    "pd.set_option('display.max_columns', None)  # To display all columns\n",
    "best_results = results.sort_values(by='Test Accuracy', ascending=False).head(5)\n",
    "print(\"Top 5 Best Performing Configurations:\")\n",
    "print(best_results)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b214caf",
   "metadata": {},
   "source": [
    "# Parameters results:\n",
    "- Initialization method: GlorotUniform \n",
    "- Activation function: Relu\n",
    "- Optimizer: RMSdrop\n",
    "- Regulazation technique: Dropout\n",
    "- -----------------------------------------\n",
    "\n",
    "# Top 5 Best Performing Configurations:\n",
    "    Optimizer  Learning Rate Momentum       Epsilon Nesterov Batch Size  \\\n",
    "    62       SGD            0.1      0.9  1.000000e-04     True         64   \n",
    "    60       SGD            0.1      0.9  1.000000e-04    False         64   \n",
    "    58       SGD            0.1      0.9  1.000000e-08     True         64   \n",
    "    50       SGD            0.1        0  1.000000e-08     True         64   \n",
    "    54       SGD            0.1        0  1.000000e-04     True         64   \n",
    "\n",
    "    Test       Loss        Test Accuracy  \n",
    "    62           0.072707         0.9780  \n",
    "    60           0.073003         0.9778  \n",
    "    58           0.070178         0.9777  \n",
    "    50           0.073058         0.9767  \n",
    "    54           0.075574         0.9765  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16efb2b",
   "metadata": {},
   "source": [
    "# Task 1.2 CNN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d477ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/2221475032.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initialization Method  Test Loss  Test Accuracy\n",
      "0                 Zeros   2.302520         0.1135\n",
      "1          RandomNormal   2.070365         0.5179\n",
      "2         RandomUniform   2.261849         0.3847\n",
      "3         GlorotUniform   0.743484         0.8445\n",
      "4          GlorotNormal   0.711782         0.8507\n",
      "5              HeNormal   0.411512         0.8987\n",
      "6             HeUniform   0.425241         0.8933\n",
      "7           LecunNormal   0.483079         0.8877\n",
      "8          LecunUniform   0.491412         0.8841\n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py + initilazation methods\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "initilization_methods = [Zeros(), \n",
    "                       RandomNormal(seed=seed_value), \n",
    "                       RandomUniform(seed=seed_value), \n",
    "                       glorot_uniform(seed=seed_value), \n",
    "                       glorot_normal(seed=seed_value), \n",
    "                       he_normal(seed=seed_value), \n",
    "                       he_uniform(seed=seed_value), \n",
    "                       lecun_normal(seed=seed_value), \n",
    "                       lecun_uniform(seed=seed_value)]\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Initialization Method', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "for method in initilization_methods:\n",
    "    # Create a new model for each initialization method\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, kernel_initializer=method))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer=method))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=method))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_initializer=method))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = {'Initialization Method': method.__class__.__name__, 'Test Loss': score[0], 'Test Accuracy': score[1]}\n",
    "    results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "930b9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/2488530854.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activation Method  Test Loss  Test Accuracy\n",
      "0              relu   0.683455         0.8465\n",
      "1           sigmoid   2.301056         0.1135\n",
      "2              tanh   0.563670         0.8680\n",
      "3            linear   0.477094         0.8779\n",
      "4           softmax   2.302552         0.1009\n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py + different activation functions\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "activation_methods = ['relu', 'sigmoid', 'tanh', 'linear', 'softmax']\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Activation Method', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "for activation_method in activation_methods:\n",
    "    # Create a new model for each activation function\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation=activation_method, input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation=activation_method))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=activation_method))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = {'Activation Method': activation_method, 'Test Loss': score[0], 'Test Accuracy': score[1]}\n",
    "    results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fd7d4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/3391922652.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Optimizer  Test Loss  Test Accuracy\n",
      "0      Adam   0.031210         0.9915\n",
      "1       SGD   0.104251         0.9665\n",
      "2   RMSprop   0.031213         0.9903\n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py + different optimizers\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Optimizer', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    # Create a new model for each optimizer\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = {'Optimizer': optimizer.__name__, 'Test Loss': score[0], 'Test Accuracy': score[1]}\n",
    "    results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8a6af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/1884897214.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Regularization Method  Test Loss  Test Accuracy\n",
      "0                  None   0.477364         0.8749\n",
      "1                    L1   5.272748         0.8681\n",
      "2                    L2   0.836787         0.8767\n",
      "3               Dropout   0.839515         0.8274\n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py + different regularization methods\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype ('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "regularization_methods = ['None', 'L1', 'L2', 'Dropout']\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Regularization Method', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "for reg_method in regularization_methods:\n",
    "    # Create a new model for each regularization method\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape, kernel_regularizer=None))\n",
    "    \n",
    "    if reg_method == 'L1':\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
    "    elif reg_method == 'L2':\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    else:\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=None))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    if reg_method == 'Dropout':\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = {'Regularization Method': reg_method, 'Test Loss': score[0], 'Test Accuracy': score[1]}\n",
    "    results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cefa4563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/28/tw9kr3yd4h950rswblzvqc9h0000gn/T/ipykernel_46211/2133874284.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Best Performing Configurations:\n",
      "     Learning Rate Momentum  Epsilon Nesterov Batch Size Regularization  \\\n",
      "115            0.1      0.9   0.0001    False         64        Dropout   \n",
      "123            0.1      0.9   0.0001     True         64        Dropout   \n",
      "88             0.1        0   0.0001     True         64           None   \n",
      "83             0.1        0   0.0001    False         64        Dropout   \n",
      "82             0.1        0   0.0001    False         64             L2   \n",
      "\n",
      "     Test Loss  Test Accuracy  \n",
      "115   0.028825         0.9912  \n",
      "123   0.029777         0.9910  \n",
      "88    0.036239         0.9899  \n",
      "83    0.031306         0.9898  \n",
      "82    0.040284         0.9897  \n",
      "     Learning Rate Momentum       Epsilon Nesterov Batch Size Regularization  \\\n",
      "0          0.00001        0  1.000000e-08    False         64           None   \n",
      "1          0.00001        0  1.000000e-08    False         64             L1   \n",
      "2          0.00001        0  1.000000e-08    False         64             L2   \n",
      "3          0.00001        0  1.000000e-08    False         64        Dropout   \n",
      "4          0.00001        0  1.000000e-08    False        256           None   \n",
      "5          0.00001        0  1.000000e-08    False        256             L1   \n",
      "6          0.00001        0  1.000000e-08    False        256             L2   \n",
      "7          0.00001        0  1.000000e-08    False        256        Dropout   \n",
      "8          0.00001        0  1.000000e-08     True         64           None   \n",
      "9          0.00001        0  1.000000e-08     True         64             L1   \n",
      "10         0.00001        0  1.000000e-08     True         64             L2   \n",
      "11         0.00001        0  1.000000e-08     True         64        Dropout   \n",
      "12         0.00001        0  1.000000e-08     True        256           None   \n",
      "13         0.00001        0  1.000000e-08     True        256             L1   \n",
      "14         0.00001        0  1.000000e-08     True        256             L2   \n",
      "15         0.00001        0  1.000000e-08     True        256        Dropout   \n",
      "16         0.00001        0  1.000000e-04    False         64           None   \n",
      "17         0.00001        0  1.000000e-04    False         64             L1   \n",
      "18         0.00001        0  1.000000e-04    False         64             L2   \n",
      "19         0.00001        0  1.000000e-04    False         64        Dropout   \n",
      "20         0.00001        0  1.000000e-04    False        256           None   \n",
      "21         0.00001        0  1.000000e-04    False        256             L1   \n",
      "22         0.00001        0  1.000000e-04    False        256             L2   \n",
      "23         0.00001        0  1.000000e-04    False        256        Dropout   \n",
      "24         0.00001        0  1.000000e-04     True         64           None   \n",
      "25         0.00001        0  1.000000e-04     True         64             L1   \n",
      "26         0.00001        0  1.000000e-04     True         64             L2   \n",
      "27         0.00001        0  1.000000e-04     True         64        Dropout   \n",
      "28         0.00001        0  1.000000e-04     True        256           None   \n",
      "29         0.00001        0  1.000000e-04     True        256             L1   \n",
      "30         0.00001        0  1.000000e-04     True        256             L2   \n",
      "31         0.00001        0  1.000000e-04     True        256        Dropout   \n",
      "32         0.00001      0.9  1.000000e-08    False         64           None   \n",
      "33         0.00001      0.9  1.000000e-08    False         64             L1   \n",
      "34         0.00001      0.9  1.000000e-08    False         64             L2   \n",
      "35         0.00001      0.9  1.000000e-08    False         64        Dropout   \n",
      "36         0.00001      0.9  1.000000e-08    False        256           None   \n",
      "37         0.00001      0.9  1.000000e-08    False        256             L1   \n",
      "38         0.00001      0.9  1.000000e-08    False        256             L2   \n",
      "39         0.00001      0.9  1.000000e-08    False        256        Dropout   \n",
      "40         0.00001      0.9  1.000000e-08     True         64           None   \n",
      "41         0.00001      0.9  1.000000e-08     True         64             L1   \n",
      "42         0.00001      0.9  1.000000e-08     True         64             L2   \n",
      "43         0.00001      0.9  1.000000e-08     True         64        Dropout   \n",
      "44         0.00001      0.9  1.000000e-08     True        256           None   \n",
      "45         0.00001      0.9  1.000000e-08     True        256             L1   \n",
      "46         0.00001      0.9  1.000000e-08     True        256             L2   \n",
      "47         0.00001      0.9  1.000000e-08     True        256        Dropout   \n",
      "48         0.00001      0.9  1.000000e-04    False         64           None   \n",
      "49         0.00001      0.9  1.000000e-04    False         64             L1   \n",
      "50         0.00001      0.9  1.000000e-04    False         64             L2   \n",
      "51         0.00001      0.9  1.000000e-04    False         64        Dropout   \n",
      "52         0.00001      0.9  1.000000e-04    False        256           None   \n",
      "53         0.00001      0.9  1.000000e-04    False        256             L1   \n",
      "54         0.00001      0.9  1.000000e-04    False        256             L2   \n",
      "55         0.00001      0.9  1.000000e-04    False        256        Dropout   \n",
      "56         0.00001      0.9  1.000000e-04     True         64           None   \n",
      "57         0.00001      0.9  1.000000e-04     True         64             L1   \n",
      "58         0.00001      0.9  1.000000e-04     True         64             L2   \n",
      "59         0.00001      0.9  1.000000e-04     True         64        Dropout   \n",
      "60         0.00001      0.9  1.000000e-04     True        256           None   \n",
      "61         0.00001      0.9  1.000000e-04     True        256             L1   \n",
      "62         0.00001      0.9  1.000000e-04     True        256             L2   \n",
      "63         0.00001      0.9  1.000000e-04     True        256        Dropout   \n",
      "64         0.10000        0  1.000000e-08    False         64           None   \n",
      "65         0.10000        0  1.000000e-08    False         64             L1   \n",
      "66         0.10000        0  1.000000e-08    False         64             L2   \n",
      "67         0.10000        0  1.000000e-08    False         64        Dropout   \n",
      "68         0.10000        0  1.000000e-08    False        256           None   \n",
      "69         0.10000        0  1.000000e-08    False        256             L1   \n",
      "70         0.10000        0  1.000000e-08    False        256             L2   \n",
      "71         0.10000        0  1.000000e-08    False        256        Dropout   \n",
      "72         0.10000        0  1.000000e-08     True         64           None   \n",
      "73         0.10000        0  1.000000e-08     True         64             L1   \n",
      "74         0.10000        0  1.000000e-08     True         64             L2   \n",
      "75         0.10000        0  1.000000e-08     True         64        Dropout   \n",
      "76         0.10000        0  1.000000e-08     True        256           None   \n",
      "77         0.10000        0  1.000000e-08     True        256             L1   \n",
      "78         0.10000        0  1.000000e-08     True        256             L2   \n",
      "79         0.10000        0  1.000000e-08     True        256        Dropout   \n",
      "80         0.10000        0  1.000000e-04    False         64           None   \n",
      "81         0.10000        0  1.000000e-04    False         64             L1   \n",
      "82         0.10000        0  1.000000e-04    False         64             L2   \n",
      "83         0.10000        0  1.000000e-04    False         64        Dropout   \n",
      "84         0.10000        0  1.000000e-04    False        256           None   \n",
      "85         0.10000        0  1.000000e-04    False        256             L1   \n",
      "86         0.10000        0  1.000000e-04    False        256             L2   \n",
      "87         0.10000        0  1.000000e-04    False        256        Dropout   \n",
      "88         0.10000        0  1.000000e-04     True         64           None   \n",
      "89         0.10000        0  1.000000e-04     True         64             L1   \n",
      "90         0.10000        0  1.000000e-04     True         64             L2   \n",
      "91         0.10000        0  1.000000e-04     True         64        Dropout   \n",
      "92         0.10000        0  1.000000e-04     True        256           None   \n",
      "93         0.10000        0  1.000000e-04     True        256             L1   \n",
      "94         0.10000        0  1.000000e-04     True        256             L2   \n",
      "95         0.10000        0  1.000000e-04     True        256        Dropout   \n",
      "96         0.10000      0.9  1.000000e-08    False         64           None   \n",
      "97         0.10000      0.9  1.000000e-08    False         64             L1   \n",
      "98         0.10000      0.9  1.000000e-08    False         64             L2   \n",
      "99         0.10000      0.9  1.000000e-08    False         64        Dropout   \n",
      "100        0.10000      0.9  1.000000e-08    False        256           None   \n",
      "101        0.10000      0.9  1.000000e-08    False        256             L1   \n",
      "102        0.10000      0.9  1.000000e-08    False        256             L2   \n",
      "103        0.10000      0.9  1.000000e-08    False        256        Dropout   \n",
      "104        0.10000      0.9  1.000000e-08     True         64           None   \n",
      "105        0.10000      0.9  1.000000e-08     True         64             L1   \n",
      "106        0.10000      0.9  1.000000e-08     True         64             L2   \n",
      "107        0.10000      0.9  1.000000e-08     True         64        Dropout   \n",
      "108        0.10000      0.9  1.000000e-08     True        256           None   \n",
      "109        0.10000      0.9  1.000000e-08     True        256             L1   \n",
      "110        0.10000      0.9  1.000000e-08     True        256             L2   \n",
      "111        0.10000      0.9  1.000000e-08     True        256        Dropout   \n",
      "112        0.10000      0.9  1.000000e-04    False         64           None   \n",
      "113        0.10000      0.9  1.000000e-04    False         64             L1   \n",
      "114        0.10000      0.9  1.000000e-04    False         64             L2   \n",
      "115        0.10000      0.9  1.000000e-04    False         64        Dropout   \n",
      "116        0.10000      0.9  1.000000e-04    False        256           None   \n",
      "117        0.10000      0.9  1.000000e-04    False        256             L1   \n",
      "118        0.10000      0.9  1.000000e-04    False        256             L2   \n",
      "119        0.10000      0.9  1.000000e-04    False        256        Dropout   \n",
      "120        0.10000      0.9  1.000000e-04     True         64           None   \n",
      "121        0.10000      0.9  1.000000e-04     True         64             L1   \n",
      "122        0.10000      0.9  1.000000e-04     True         64             L2   \n",
      "123        0.10000      0.9  1.000000e-04     True         64        Dropout   \n",
      "124        0.10000      0.9  1.000000e-04     True        256           None   \n",
      "125        0.10000      0.9  1.000000e-04     True        256             L1   \n",
      "126        0.10000      0.9  1.000000e-04     True        256             L2   \n",
      "127        0.10000      0.9  1.000000e-04     True        256        Dropout   \n",
      "\n",
      "     Test Loss  Test Accuracy  \n",
      "0     2.297915         0.1597  \n",
      "1     9.835915         0.1715  \n",
      "2     2.726040         0.1277  \n",
      "3     2.302328         0.1471  \n",
      "4     2.297942         0.1361  \n",
      "5     9.957870         0.0853  \n",
      "6     2.719232         0.1655  \n",
      "7     2.290014         0.1540  \n",
      "8     2.299754         0.0999  \n",
      "9     9.859148         0.1008  \n",
      "10    2.724669         0.1034  \n",
      "11    2.303691         0.1754  \n",
      "12    2.305553         0.1200  \n",
      "13    9.959502         0.0866  \n",
      "14    2.728338         0.0910  \n",
      "15    2.304857         0.0852  \n",
      "16    2.270077         0.1931  \n",
      "17    9.738099         0.1193  \n",
      "18    2.705228         0.1202  \n",
      "19    2.270783         0.2477  \n",
      "20    2.295165         0.0927  \n",
      "21    9.887589         0.1125  \n",
      "22    2.723660         0.1456  \n",
      "23    2.291225         0.1441  \n",
      "24    2.290549         0.1462  \n",
      "25    9.710016         0.2073  \n",
      "26    2.718306         0.1641  \n",
      "27    2.280217         0.2121  \n",
      "28    2.297324         0.1501  \n",
      "29    9.896837         0.1257  \n",
      "30    2.721590         0.0932  \n",
      "31    2.295494         0.1031  \n",
      "32    2.298847         0.1031  \n",
      "33    9.911416         0.1348  \n",
      "34    2.716369         0.1025  \n",
      "35    2.305767         0.1208  \n",
      "36    2.307561         0.1511  \n",
      "37   10.004410         0.1348  \n",
      "38    2.725197         0.0870  \n",
      "39    2.301293         0.1192  \n",
      "40    2.306426         0.0933  \n",
      "41    9.892643         0.1515  \n",
      "42    2.725370         0.0666  \n",
      "43    2.296573         0.1192  \n",
      "44    2.294721         0.1325  \n",
      "45   10.019318         0.1041  \n",
      "46    2.733859         0.0419  \n",
      "47    2.296253         0.1313  \n",
      "48    2.286165         0.1754  \n",
      "49    9.751274         0.1332  \n",
      "50    2.713088         0.1822  \n",
      "51    2.280511         0.1069  \n",
      "52    2.286493         0.1316  \n",
      "53    9.947986         0.1101  \n",
      "54    2.735244         0.0990  \n",
      "55    2.309018         0.0632  \n",
      "56    2.266228         0.3022  \n",
      "57    9.765157         0.1708  \n",
      "58    2.706405         0.2049  \n",
      "59    2.282598         0.1684  \n",
      "60    2.300622         0.1024  \n",
      "61    9.918027         0.1311  \n",
      "62    2.716567         0.1542  \n",
      "63    2.294461         0.0798  \n",
      "64    0.068568         0.9777  \n",
      "65    0.264200         0.9434  \n",
      "66    0.094569         0.9754  \n",
      "67    0.094102         0.9714  \n",
      "68    0.087026         0.9750  \n",
      "69    0.325492         0.9339  \n",
      "70    0.160353         0.9699  \n",
      "71    0.118279         0.9634  \n",
      "72    0.065309         0.9796  \n",
      "73    0.263505         0.9410  \n",
      "74    0.092833         0.9759  \n",
      "75    0.097087         0.9697  \n",
      "76    0.086171         0.9740  \n",
      "77    0.338595         0.9297  \n",
      "78    0.162478         0.9684  \n",
      "79    0.126911         0.9621  \n",
      "80    0.036494         0.9887  \n",
      "81    0.200991         0.9747  \n",
      "82    0.040284         0.9897  \n",
      "83    0.031306         0.9898  \n",
      "84    0.032347         0.9891  \n",
      "85    0.214804         0.9691  \n",
      "86    0.073260         0.9789  \n",
      "87    0.033216         0.9887  \n",
      "88    0.036239         0.9899  \n",
      "89    0.204938         0.9721  \n",
      "90    0.043632         0.9889  \n",
      "91    0.037418         0.9883  \n",
      "92    0.034240         0.9881  \n",
      "93    0.213009         0.9694  \n",
      "94    0.055557         0.9862  \n",
      "95    0.037107         0.9867  \n",
      "96    0.054764         0.9816  \n",
      "97    0.241056         0.9559  \n",
      "98    0.097772         0.9802  \n",
      "99    0.073613         0.9751  \n",
      "100   0.080993         0.9764  \n",
      "101   0.352450         0.9390  \n",
      "102   0.177211         0.9692  \n",
      "103   0.124345         0.9626  \n",
      "104   0.051648         0.9838  \n",
      "105   0.236345         0.9581  \n",
      "106   0.097662         0.9801  \n",
      "107   0.078926         0.9758  \n",
      "108   0.087532         0.9740  \n",
      "109   0.364574         0.9332  \n",
      "110   0.176055         0.9685  \n",
      "111   0.126548         0.9626  \n",
      "112   0.038103         0.9888  \n",
      "113   0.188085         0.9754  \n",
      "114   0.051786         0.9857  \n",
      "115   0.028825         0.9912  \n",
      "116   0.047910         0.9841  \n",
      "117   0.208282         0.9713  \n",
      "118   0.060880         0.9844  \n",
      "119   0.036848         0.9868  \n",
      "120   0.039508         0.9895  \n",
      "121   0.182066         0.9783  \n",
      "122   0.048069         0.9880  \n",
      "123   0.029777         0.9910  \n",
      "124   0.031641         0.9894  \n",
      "125   0.217054         0.9693  \n",
      "126   0.084925         0.9756  \n",
      "127   0.039303         0.9871  \n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py + different hyperparameters\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define hyperparameter values to test\n",
    "learning_rates = [0.00001, 0.1]\n",
    "momentum_values = [0, 0.9]\n",
    "epsilon_values = [1e-8, 1e-4]\n",
    "nesterov_values = [False, True]\n",
    "batch_sizes = [64, 256]\n",
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = pd.DataFrame(columns=['Learning Rate', 'Momentum', 'Epsilon', 'Nesterov', 'Batch Size', 'Regularization', 'Test Loss', 'Test Accuracy'])\n",
    "\n",
    "# Test different hyperparameters\n",
    "for lr, momentum, epsilon, nesterov, batch_size, reg_method in product(learning_rates, momentum_values, epsilon_values, nesterov_values, batch_sizes, regularization_methods):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1), kernel_regularizer=None))\n",
    "    \n",
    "    if reg_method == 'L1':\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l1(0.01)))\n",
    "    elif reg_method == 'L2':\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    else:\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=None))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    if reg_method == 'Dropout':\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = keras.optimizers.Adadelta(learning_rate=lr, rho=momentum, epsilon=epsilon)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    new_row = {'Learning Rate': lr, 'Momentum': momentum, 'Epsilon': epsilon, 'Nesterov': nesterov, 'Batch Size': batch_size, 'Regularization': reg_method, 'Test Loss': score[0], 'Test Accuracy': score[1]}\n",
    "    results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "pd.set_option('display.max_rows', None)  # To display all rows\n",
    "pd.set_option('display.max_columns', None)  # To display all columns \n",
    "best_results = results.sort_values(by='Test Accuracy', ascending=False).head(5)\n",
    "print(\"Top 5 Best Performing Configurations:\")\n",
    "print(best_results)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806ca49",
   "metadata": {},
   "source": [
    "# Parameters results:\n",
    "- Initialization method: HeNormal\n",
    "- Activation function: linear\n",
    "- Optimizer: Adam\n",
    "- Regulazation technique: L2\n",
    "- -----------------------------------------\n",
    "# Top 5 Best Performing Configurations:\n",
    "        Learning Rate Momentum  Epsilon Nesterov Batch Size Regularization\n",
    "    115            0.1      0.9   0.0001    False         64        Dropout   \n",
    "    123            0.1      0.9   0.0001     True         64        Dropout   \n",
    "    88             0.1        0   0.0001     True         64           None   \n",
    "    83             0.1        0   0.0001    False         64        Dropout   \n",
    "    82             0.1        0   0.0001    False         64             L2   \n",
    "\n",
    "\n",
    "     Test Loss  Test Accuracy  \n",
    "    115   0.028825         0.9912  \n",
    "    123   0.029777         0.9910  \n",
    "    88    0.036239         0.9899  \n",
    "    83    0.031306         0.9898  \n",
    "    82    0.040284         0.9897 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
